= Hazelcast 集群管理器

Vert.x 基于 https://hazelcast.com[Hazelcast] 实现了一个集群管理器。

这是 Vert.x CLI默认的集群管理器。由于 Vert.x 集群管理器的可插拔性，可轻易切换至其它的集群管理器。

这个集群管理器由以下依赖引入：

[source,xml,subs="+attributes"]
----
<dependency>
 <groupId>io.vertx</groupId>
 <artifactId>vertx-hazelcast</artifactId>
 <version>4.0.0</version>
</dependency>
----

Vert.x 集群管理器包含以下几项功能：

* 发现并管理集群中的节点
* 管理集群的 EventBus 话题订阅清单（这样就可以轻松得知集群中的哪些节点订阅了哪些 EventBus 地址）
* 分布式 Map 支持
* 分布式锁
* 分布式计数器

Vert.x 集群器 *并不* 处理节点之间的通信。在 Vert.x 中，集群节点间通信是直接由 TCP 连接处理的。

[[_using_this_cluster_manager]]
== 使用集群管理器

如果通过命令行来使用 Vert.x，对应集群管理器 `jar` 包（名为 `vertx-hazelcast-4.0.0.jar` ） 应该在 Vert.x 中安装目录的 `lib` 目录中。

如果在 Maven 或者 Gradle 工程中使用 Vert.x， 只需要在工程依赖中加上依赖：`io.vertx:vertx-hazelcast:4.0.0`。

如果（集群管理器的）jar 包在 classpath 中，Vert.x将自动检测到，并将其作为集群管理器。 需要注意的是，要确保 Vert.x 的 classpath 中没有其它的集群管理器实现，否则会使用错误的集群管理器。

在内嵌 Vert.x 时，可以在创建 Vert.x 实例时通过编程的方式显式配置 Vert.x 集群管理器， 比如：

[source,java]
----
ClusterManager mgr = new HazelcastClusterManager();

VertxOptions options = new VertxOptions().setClusterManager(mgr);

Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // 失败
  }
});
----

[[configcluster]]
== 配置集群管理器

[[_configuring_with_xml]]
=== 使用XML文件配置

通常情况下，集群管理器的相关配置是通过打包在jar中的默认配置文件
https://github.com/vert-x3/vertx-hazelcast/blob/master/src/main/resources/default-cluster.xml[`default-cluster.xml`]
配置的。

如果要覆盖此配置，可以在 classpath 中添加一个 `cluster.xml` 文件。如果想在 fat jar 中内嵌 `cluster.xml` ，此文件必须在 fat jar 的根目录中。如果此文件是一个外部文件，则必须将其所在的 **目录** 添加至 classpath 中。举个例子，如果使用 Vert.x 的 _launcher_ 类，则 classpath 应该设置为：

[source]
----
# 如果 cluster.xml 在当前目录：
java -jar ... -cp . -cluster
vertx run MyVerticle -cp . -cluster

# 如果 cluster.xml 在 conf 目录：
java -jar ... -cp conf -cluster
----

还可以通过系统配置 `vertx.hazelcast.config`
来覆盖默认的配置文件：

[source]
----
# 指定一个外部文件为自定义配置文件
java -Dvertx.hazelcast.config=./config/my-cluster-config.xml -jar ... -cluster

# 从 classpath 中加载一个文件为自定义配置文件
java -Dvertx.hazelcast.config=classpath:my/package/config/my-cluster-config.xml -jar ... -cluster
----

如果 `vertx.hazelcast.config` 值不为空时，将覆盖 classpath 中所有的 `cluster.xml` 文件， 但是如果加载 `vertx.hazelcast.config` 系统配置失败时， 系统将选取 classpath 任意一个 `cluster.xml` ，甚至直接使用默认配置。

CAUTION: Vert.x 并不支持 `-Dhazelcast.config` 设置方式， 请不要使用。

这里的 xml 是 Hazelcast 的配置文件， 可以在 Hazelcast 官网找到详细的配置文档。

[[_configuring_programmatically]]
=== 通过编程配置

您也可以通过编程的形式配置集群管理器：

[source,java]
----
Config hazelcastConfig = new Config();

// 设置相关的hazlcast配置，在这里省略掉，不再赘述

ClusterManager mgr = new HazelcastClusterManager(hazelcastConfig);

VertxOptions options = new VertxOptions().setClusterManager(mgr);

Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // 失败！
  }
});
----

您也可以对已存在的XML配置进行修改。 比如修改集群名：

[source,java]
----
Config hazelcastConfig = ConfigUtil.loadConfig();

hazelcastConfig.setClusterName("my-cluster-name");

ClusterManager mgr = new HazelcastClusterManager(hazelcastConfig);

VertxOptions options = new VertxOptions().setClusterManager(mgr);

Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // 失败！
  }
});
----

`ConfigUtil#loadConfig` 方法加载 Hazelcast 的XML配置文件并将其转换为 `Config` 对象。 读取的XML配置文件来自：

. `vertx.hazelcast.config` 系统配置指定的文件（若存在），或
. classpath 内的 `cluster.xml` 文件（若存在），或
. 默认的配置文件

[[_discovery_options]]
=== 发现配置

Hazelcast 支持几种不同的发现配置。 Hazelcast 默认配置使用多播，因此您必须在网络上启用多播才能正常工作。

有关如何配置不同的发现方式，请查阅 Hazelcast 文档。

[[_changing_local_and_public_address_with_system_properties]]
=== 通过系统配置改变本地地址及公共地址

有时，群集节点必须绑定到其他集群成员无法访问的地址。 例如，节点不在同一网络区域中，或在某些具有特定防火墙配置的云服务中时，可能会发生这种情况。

可以使用以下系统属性设置绑定本地地址和公共地址（向其他成员发布的地址）：

----
-Dhazelcast.local.localAddress=172.16.5.131 -Dhazelcast.local.publicAddress=104.198.78.81
----

[[_using_an_existing_hazelcast_cluster]]
== 使用已存在的 Hazelcast 集群

可以向集群管理器传入 `HazelcastInstance` 来复用现有集群：

[source,java]
----
ClusterManager mgr = new HazelcastClusterManager(hazelcastInstance);
VertxOptions options = new VertxOptions().setClusterManager(mgr);
Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // 失败！
  }
});
----

在这种情况下，Vert.x不是 Hazelcast 群集的所有者，所以不要关闭 Vert.x 时关闭 Hazlecast 集群。

请注意，自定义 Hazelcast 实例需要以下配置：

[source,xml]
----
<multimap name="__vertx.subs">
 <backup-count>1</backup-count>
 <value-collection-type>SET</value-collection-type>
</multimap>

<map name="__vertx.haInfo">
 <backup-count>1</backup-count>
</map>

<map name="__vertx.nodeInfo">
 <backup-count>1</backup-count>
</map>

<cp-subsystem>
 <cp-member-count>0</cp-member-count>
 <semaphores>
   <semaphore>
     <name>__vertx.*</name>
     <jdk-compatible>false</jdk-compatible>
     <initial-permits>1</initial-permits>
   </semaphore>
 </semaphores>
</cp-subsystem>
----

IMPORTANT: 不支持 Hazelcast 客户端及智能客户端。

IMPORTANT: 要确保 Hazelcast 集群 先于 Vert.x 集群启动，后于 Vert.x 集群关闭。 同时需要禁用 `shutdown hook` （参考上述的 xml 配置，或通过系统变量来实现）。

[[_changing_timeout_for_failed_nodes]]
== 修改故障节点的超时配置

缺省情况下，Hazelcast 会移除集群中超过300秒没收到心跳的节点。 通过系统配置 `hazelcast.max.no.heartbeat.seconds` 可以修改这个超时时间，如:

----
-Dhazelcast.max.no.heartbeat.seconds=5
----

修改后，超过5秒没发出心跳的节点会被移出集群。

请参考 https://docs.hazelcast.org/docs/latest/manual/html-single/#system-properties[Hazelcast 系统配置] 。

[[_trouble_shooting_clustering]]
== 集群故障排除

如果默认的组播配置不能正常运行，通常有以下原因：

[[_multicast_not_enabled_on_the_machine]]
=== 机器禁用组播

通常来说，OSX 默认禁用组播。 请自行Google一下如何启用组播。

[[_using_wrong_network_interface]]
=== 使用错误的网络接口

如果机器上有多个网络接口（也有可能是在运行 VPN 的情况下），那么 Hazelcast 很有可能是使用了错误的网络接口。

为了确保 Hazelcast 使用正确的网络接口，在配置文件中将 `interface`
设置为指定IP地址.
同时确保 `enabled` 属性设置为 `true` 。
例如：

----
<interfaces enabled="true">
 <interface>192.168.1.20</interface>
</interfaces>
----

[[_using_a_vpn]]
=== 使用VPN

VPN 软件工作时通常会创建虚拟网络接口，往往不支持组播。
在 VPN 环境中，如果 Hazelcast 与 Vert.x 不正确配置的话，
将会选择 VPN 创建的网络接口，而不是正确的网络接口。

所以，如果你的应用运行在 VPN 环境中，参考上述章节，
设置正确的网络接口。

[[_when_multicast_is_not_available]]
=== 组播不可用

在某些情况下，因为特殊的运行环境，可能无法使用组播。
在这种情况下，应该配置其他网络传输，例如使用 TCP 套接字，在亚马逊云 EC2 上使用AWS。

有关 Hazelcast 更多传输方式，以及如何配置它们，
请查询 Hazelcast 文档。

[[_enabling_logging]]
=== 开启日志

在排除故障时，开启 Hazelcast 日志很有帮助，可以观察是否组成了集群。
使用默认的 JUL 日志时，在 classpath 中添加 `vertx-default-jul-logging.properties` 文件可开启 Hazelcast 日志。
这是一个标准 java.util.logging（JUL） 配置文件。
具体配置如下：

----
com.hazelcast.level=INFO
----

以及

----
java.util.logging.ConsoleHandler.level=INFO
java.util.logging.FileHandler.level=INFO
----

[[_hazelcast_logging]]
== Hazelcast 日志配置

Hazelcast 的日志默认采用 `JDK` 的实现（即 JUL）。
如果想切换至其他日志库，通过设置系统配置 `hazelcast.logging.type` 即可：

----
-Dhazelcast.logging.type=slf4j
----

详细文档请参考 http://docs.hazelcast.org/docs/3.6.1/manual/html-single/index.html#logging-configuration[hazelcast 文档] 。

[[_using_a_different_hazelcast_version]]
== 使用其他 Hazelcast 版本

当前的 Vert.x HazelcastClusterManager 使用的 Hazelcast 版本为 `4.0.2` 。
如果开发者想使用其他版本的 Hazelcast，需要做以下工作：

* 将目标版本的 Hazelcast 依赖添加至 classpath 中
* 如果是 fat jar 的形式，在构建工具中使用正确的版本

使用 Maven 时可参考下面代码：

[source,xml,subs="+attributes"]
----
<dependency>
 <groupId>com.hazelcast</groupId>
 <artifactId>hazelcast</artifactId>
 <version>ENTER_YOUR_VERSION_HERE</version>
</dependency>
<dependency>
 <groupId>io.vertx</groupId>
 <artifactId>vertx-hazelcast</artifactId>
 <version>4.0.0</version>
</dependency>
----

对于某些版本，您可能需要排除掉一些（冲突的）依赖。

对于 Gradle 可以使用下面代码:

[source]
----
dependencies {
compile ("io.vertx:vertx-hazelcast:4.0.0"){
  exclude group: 'com.hazelcast', module: 'hazelcast'
}
compile "com.hazelcast:hazelcast:ENTER_YOUR_VERSION_HERE"
}
----

== Configuring for Kubernetes

On Kubernetes, Hazelcast should be configured to use the https://github.com/hazelcast/hazelcast-kubernetes[Hazelcast Kubernetes] plugin.

First, add the `io.vertx:vertx-hazelcast:${vertx.version}` and `com.hazelcast:hazelcast-kubernetes:${hazelcast-kubernetes.version}` dependencies to your project.
With Maven it looks like:

[source,xml]
----
<dependency>
 <groupId>io.vertx</groupId>
 <artifactId>vertx-hazelcast</artifactId>
 <version>${vertx.version}</version>
</dependency>
<dependency>
 <groupId>com.hazelcast</groupId>
 <artifactId>hazelcast-kubernetes</artifactId>
 <version>${hazelcast-kubernetes.version}</version>
</dependency>
----

NOTE: If you use a different version of the Hazelcast core library, make sure to use a compatible version of the Kubernetes discovery plugin.

The second step is to configure the discovery plugin inside of your Hazelcast configuration, by either providing a custom `cluster.xml` file or programmatically, as described in <<configcluster>>.

The plugin provides two https://github.com/hazelcast/hazelcast-kubernetes#understanding-discovery-modes[discovery modes]: _Kubernetes API_ and _DNS Lookup_.
Please refer to the plugin project page for pros and cons of both modes.

In this document, we will use _DNS Lookup_ discovery. The following properties have to be changed / added:

[source,xml]
----
<hazelcast>
 <properties>
   <property name="hazelcast.discovery.enabled">true</property> <!--1-->
 </properties>

 <network>
   <join>
     <multicast enabled="false"/> <!--2-->
     <tcp-ip enabled="false" />

     <discovery-strategies>
       <discovery-strategy enabled="true"> <!--3-->
           class="com.hazelcast.kubernetes.HazelcastKubernetesDiscoveryStrategy">
         <properties>
           <property name="service-dns">MY-SERVICE-DNS-NAME</property> <!--4-->
         </properties>
       </discovery-strategy>
     </discovery-strategies>
   </join>
 </network>
</hazelcast>
----
<1> Activate Discovery SPI
<2> Deactivate other discoveries
<3> Activate the Kubernetes plugin
<4> Service DNS, usually in the form of `MY-SERVICE-NAME.MY-NAMESPACE.svc.cluster.local` but depends on the Kubernetes distribution

The `MY-SERVICE-DNS-NAME` value must be a *headless* Kubernetes service name that will be used by Hazelcast to identify all cluster members.
A headless service can be created with:

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
 namespace: MY-NAMESPACE
 name: MY-SERVICE-NAME
spec:
 selector:
   component: MY-SERVICE-NAME # <1>
 clusterIP: None
 ports:
 - name: hz-port-name
   port: 5701
   protocol: TCP
----
<1> Cluster members selected by label

Eventually, attach the `component` label to all deployments that should be part of the cluster:

[source,yaml]
----
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
 namespace: MY-NAMESPACE
spec:
 template:
   metadata:
     labels:
       component: MY-SERVICE-NAME
----

Further configuration details are available on the https://github.com/hazelcast/hazelcast-kubernetes[Hazelcast Kubernetes Plugin page].

=== Rolling updates

During rolling updates, it is recommended to replace pods one by one.

To do so, we must configure Kubernetes to:

* never start more than one new pod at once
* forbid more than one unavailable pod during the process

[source,yaml]
----
spec:
 strategy:
   type: Rolling
   rollingParams:
     updatePeriodSeconds: 10
     intervalSeconds: 20
     timeoutSeconds: 600
     maxUnavailable: 1 <1>
     maxSurge: 1 <2>
----
<1> the maximum number of pods that can be unavailable during the update process
<2> the maximum number of pods that can be created over the desired number of pods

Also, the pod readiness probe must take the cluster state into account.
Please refer to the <<one-by-one, cluster administration>> section for details on how to implement a readiness probe with link:../../vertx-health-check/java/[Vert.x Health Checks].

== Cluster administration

The Hazelcast cluster manager works by turning Vert.x nodes into members of a Hazelcast cluster.
As a consequence, Vert.x cluster manager administration should follow the Hazelcast management guidelines.

First, let's take a step back and introduce data partitioning and split-brain syndrome.

=== Data partitioning

Each Vert.x node holds pieces of the clustering data: eventbus subscriptions, async map entries, clustered counters... etc.

When a member joins or leaves the cluster, Hazelcast migrates data partitions.
In other words, it moves data around to accomodate the new cluster topology.
This process may take some time, depending on the amount of clustered data and number of nodes.

=== Split-brain syndrome

In a perfect world, there would be no network equipment failures.
Reality is, though, that sooner or later your cluster will be divided into smaller groups, unable to see each others.

Hazelcast is capable of merging the nodes back into a single cluster.
But just as with data partition migrations, this process may take some time.
Before the cluster is fully functional again, some eventbus consumers might not be able to get messages.
Or high-availability may not be able to redeploy a failing verticle.

[NOTE]
====
It is difficult (if possible at all) to make a difference between a network partition and:

- long GC pauses (leading to missed heartbeats checks),
- many nodes being killed forcefully, at-once, because you are deploying a new version of your application
====

=== Recommendations

Considering the common clustering issues discussed above, it is recommended to stick to the following good practices.

==== Graceful shutdown

Avoid stopping members forcefully (e.g, `kill -9` a node).

Of course process crashes are inevitable, but a graceful shutdown helps to get the remaining nodes in a stable state faster.

[[one-by-one]]
==== One node after the other

When rolling a new version of your app, scaling-up or down your cluster, add or remove nodes one after the other.

Stopping nodes one by one prevents the cluster from thinking a network partition occured.
Adding them one by one allows for clean, incremental data partition migrations.

The cluster safety can be verified with link:../../vertx-health-check/java/[Vert.x Health Checks]:

[source,java]
----
Handler<Promise<Status>> procedure = ClusterHealthCheck.createProcedure(vertx);
HealthChecks checks = HealthChecks.create(vertx).register("cluster-health", procedure);
----

After creation, the health check can be exposed over HTTP with a link:../../vertx-web/java/[Vert.x Web] router handler:

[source,java]
----
Router router = Router.router(vertx);
router.get("/readiness").handler(HealthCheckHandler.createWithHealthChecks(checks));
----

==== Using Lite Members

To minimize the time a Vert.x cluster spends accomodating a new topology, you may use external data nodes and mark Vert.x nodes as https://docs.hazelcast.org/docs/latest/manual/html-single/#enabling-lite-members[_Lite Members_].

_Lite Members_ participate in a Hazelcast cluster like regular members, but they do not own any data partition.
Therefore, Hazelcast does not need to migrate partitions when such members are added or removed.

IMPORTANT: You must start the external data nodes beforehand as Hazelcast won't create a cluster with _Lite Members_ only.

To start an external node, you can use the Hazelcast distribution start script, or proceed programmatically.

Vert.x nodes can be marked as _Lite Members_ in the XML configuration:

[source,xml]
----
<lite-member enabled="true"/>
----

You can also do it programmatically:

[source,java]
----
Config hazelcastConfig = ConfigUtil.loadConfig()
  .setLiteMember(true);

ClusterManager mgr = new HazelcastClusterManager(hazelcastConfig);

VertxOptions options = new VertxOptions().setClusterManager(mgr);

Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // failed!
  }
});
----