= Infinispan 集群管理器

本项目基于 https://infinispan.org/[Infinispan] 实现了一个集群管理器。

这个集群管理器的实现由以下依赖引入：

[source,xml,subs="+attributes"]
----
<dependency>
 <groupId>io.vertx</groupId>
 <artifactId>vertx-infinispan</artifactId>
 <version>4.0.0</version>
</dependency>
----

Vert.x 集群管理器包含以下几项功能：

* 发现并管理集群中的节点
* 管理集群的 EventBus 地址订阅清单（这样就可以轻松得知集群中的哪些节点订阅了哪些 EventBus 地址）
* 分布式 Map 支持
* 分布式锁
* 分布式计数器

Vert.x 集群器 *并不* 处理节点之间的通信。在 Vert.x 中，集群节点间通信是直接由 TCP 连接处理的。

[[_using_this_cluster_manager]]
== 使用集群管理器

如果通过命令行来使用 Vert.x，对应集群管理器的 `jar` 包（名为 `vertx-infinispan-4.0.0.jar` ）
应该在 Vert.x 中安装路径的 `lib` 目录中。

如果在 Maven 或者 Gradle 工程中使用 Vert.x， 只需要在工程依赖中加上依赖：
`io.vertx:vertx-infinispan:4.0.0`。

如果（集群管理器的）jar 包在 classpath 中，Vert.x将自动检测到并将其作为集群管理器。
需要注意的是，要确保 Vert.x 的 classpath 中没有其它的集群管理器实现，
否则会使用错误的集群管理器。

编程内嵌 Vert.x 时，可以在创建 Vert.x 实例时，
通过编程的方式显式配置 Vert.x 集群管理器，例如：

[source,java]
----
ClusterManager mgr = new InfinispanClusterManager();

VertxOptions options = new VertxOptions().setClusterManager(mgr);

Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // 失败！
  }
});
----

[[_configuring_this_cluster_manager]]
== 配置集群管理器

默认的集群管理器配置可以通过 `infinispan.xml` 及/或 `jgroups.xml` 文件进行修改。
前者配置数据网格（data grid），后者配置组管理和集群成员发现。

您可以在 classpath 中替换这些配置文件（其中一个或两个）。
如果想在 fat jar 中内嵌自己的配置文件，此文件必须在 fat jar 的根目录中。
如果此文件是一个外部文件，则必须将其所在的 **目录** 添加至 classpath 中。
举个例子，如果使用 Vert.x 的 _launcher_ 启动应用，则 classpath 应该设置为：

[source,shell]
----
# 如果 infinispan.xml 及/或 jgroups.xml 在当前目录：
java -jar my-app.jar -cp . -cluster

# 如果 infinispan.xml 及/或 jgroups.xml 在 conf 目录：
java -jar my-app.jar -cp conf -cluster
----

还可以通过配置系统属性 `vertx.infinispan.config` 及/或 `vertx.jgroups.config`
指定配置文件来覆盖默认配置：

[source,shell]
----
# 指定一个外部文件为自定义配置文件
java -Dvertx.infinispan.config=./config/my-infinispan.xml -jar ... -cluster

# 或从 classpath 中加载一个文件为自定义配置文件
java -Dvertx.infinispan.config=my/package/config/my-infinispan.xml -jar ... -cluster
----

集群管理器优先在 classpath 查找指定的配置文件，如果没有找到，则在文件系统中查找指定的配置文件。

如果设定了上述的系统属性，则会覆盖 classpath 中的 `infinispan.xml` or `jgroups.xml` 文件。

`jgroup.xml` 与 `infinispan.xml` 分别是 JGroups 、 Infinispan 配置文件。在对应的官方可以网站可以详细的配置攻略。

IMPORTANT: 如果 classpath 中包含 `jgroups.xml` 文件，同时也设置了 `vertx.jgroups.config` 系统属性，
那么 Infinispan 配置中所有的 JGroups 的 `stack-file` 路径配置会被 `jgroups.xml` 中的覆盖。

在默认的 JGroups 配置中，节点发现使用组播，组管理使用 TCP 。
请确认您的网络支持组播。

有关如何配置或使用其他传输方式的完整文档，
请查询 Infinispan 或 JGroups 文档。

[[_using_an_existing_infinispan_cache_manager]]
== 使用已有的 Infinispan 缓存管理器

构造集群管理器时，传入已有的 `DefaultCacheManager` 可以复用已有的缓存管理器。

[source,java]
----
ClusterManager mgr = new InfinispanClusterManager(cacheManager);

VertxOptions options = new VertxOptions().setClusterManager(mgr);

Vertx.clusteredVertx(options, res -> {
  if (res.succeeded()) {
    Vertx vertx = res.result();
  } else {
    // 失败！
  }
});
----

在这种情况下，Vert.x 并不是缓存管理器的所有者，因此不能在关闭 Vert.x 时停止 Infinispan 。

需要注意的是，需要通过如下配置来自定义 Infinispan 实例：

[source,xml]
----
<cache-container default-cache="distributed-cache">
 <distributed-cache name="distributed-cache"/>
 <replicated-cache name="__vertx.subs"/>
 <replicated-cache name="__vertx.haInfo"/>
 <replicated-cache name="__vertx.nodeInfo"/>
 <distributed-cache-configuration name="__vertx.distributed.cache.configuration"/>
</cache-container>
----

== Packaging an executable uber JAR

Infinispan uses Java's `ServiceLoader` mechanism to discover implementations of a few classes at runtime.

You must configure your build tool to merge service descriptors files when creating an executable uber JAR (also known as "fat" JAR).

If you use Maven and the Maven Shade Plugin, the plugin configuration should look like:

[source,xml]
----
<configuration>
 <transformers>
   <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
   <!-- ... -->
 </transformers>
 <!-- ... -->
</configuration>
----

If you use Gradle and the Gradle Shadow Plugin:

[source,kotlin]
----
shadowJar {
 mergeServiceFiles()
}
----

== Configuring for Kubernetes

On Kubernetes, JGroups can be configured to use either the Kubernetes API (`KUBE_PING`) or DNS (`DNS_PING`) for discovery.
In this document, we will use DNS discovery.

First, force usage of IPv4 in the JVM with a system property.

[source,shell]
----
-Djava.net.preferIPv4Stack=true
----

Then, set the `vertx.jgroups.config` system property to `default-configs/default-jgroups-kubernetes.xml`.
This JGroups stack file is located in the `infinispan-core` JAR and preconfigured for Kubernetes.

[source,shell]
----
-Dvertx.jgroups.config=default-configs/default-jgroups-kubernetes.xml
----

Also, set the JGroups DNS query to find members.

[source,shell]
----
-Djgroups.dns.query=MY-SERVICE-DNS-NAME
----

The `MY-SERVICE-DNS-NAME` value must be a https://kubernetes.io/docs/user-guide/services/#headless-services[*headless* Kubernetes service] name that will be used by JGroups to identify all cluster members.
A headless service can be created with:

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
 name: clustered-app
spec:
 selector:
   cluster: clustered-app # <2>
 ports:
   - name: jgroups
     port: 7800 # <1>
     protocol: TCP
 publishNotReadyAddresses: true # <3>
 clusterIP: None
----
<1> JGroups TCP port
<2> Cluster members selected by the `cluster=clustered-app` label
<3> Set to true so that members can be discovered without interfering with your readiness probe logic

Eventually, apply the `cluster=clustered-app` label to all deployments that should be part of the cluster:

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
spec:
 template:
   metadata:
     labels:
       cluster: clustered-app
----

=== Rolling updates

During rolling udpates, the Infinispan team http://infinispan.org/docs/stable/user_guide/user_guide.html#using_kubernetes_and_openshift_rolling_updates[recommends] to replace pods one by one.

To do so, we must configure Kubernetes to:

* never start more than one new pod at once
* forbid more than one unavailable pod during the process

[source,yaml]
----
spec:
 strategy:
   type: Rolling
   rollingParams:
     updatePeriodSeconds: 10
     intervalSeconds: 20
     timeoutSeconds: 600
     maxUnavailable: 1 <1>
     maxSurge: 1 <2>
----
<1> the maximum number of pods that can be unavailable during the update process
<2> the maximum number of pods that can be created over the desired number of pods

Also, pod readiness probe must take cluster health into account.
Please refer to the <<one-by-one, cluster administration>> section for details on how to implement a readiness probe with link:../../vertx-health-check/java/[Vert.x Health Checks].

[[_configuring_for_docker_compose]]
== 适配 Docker Compose

确认 JVM 在启动时 设置了下面的配置：

[source,shell]
----
-Djava.net.preferIPv4Stack=true -Djgroups.bind.address=NON_LOOPBACK
----

通过上述两项系统配置，JGroups 才能正确地选择 Docker 创建的虚拟网络接口。

[[_trouble_shooting_clustering]]
== 集群故障排查

如果默认的组播配置不能正常运行，通常有以下原因：

[[_multicast_not_enabled_on_the_machine]]
=== 机器禁用组播

通常来说，OSX 默认禁用组播。
请自行Google一下如何启用组播。

[[_using_wrong_network_interface]]
=== 使用错误的网络接口

如果机器上有多个网络接口（也有可能是在运行 VPN 的情况下），
那么 JGroups 很有可能使用错误的网络接口。

为了确保 JGroups 使用正确的网络接口，在配置文件中将 `bind_addr` 设置为指定IP地址。
例如：

[source,xml]
----
<TCP bind_addr="192.168.1.20"
    ...
    />
<MPING bind_addr="192.168.1.20"
    ...
    />
----

如果您直接使用了内置的 `jgroups.xml` 配置文件，也可以通过设置 `jgroups.bind.address` 系统属性来指定 JGroups 的网络接口：

----
-Djgroups.bind.address=192.168.1.20
----

Vert.x 运行在集群模式时，必须确保 Vert.x 获取到正确的网络接口。
在 Vert.x 命令行模式下，可以通过 `cluster-host` 选项指定集群的网络接口：

----
vertx run myverticle.js -cluster -cluster-host your-ip-address
----

其中 `your-ip-address` 与 JGroups 配置中指定的IP地址一致。

若使用编码的方式启动 Vert.x，可以通过 `link:../../apidocs/io/vertx/core/VertxOptions.html#getEventBusOptions--[.setHost(java.lang.String)]` 设置集群的网络接口。

[[_using_a_vpn]]
=== 使用VPN

使用VPN是上述问题的变种。
VPN 软件工作时通常会创建虚拟网络接口，但往往不支持组播。
在 VPN 环境中，如果 JGroups 与 Vert.x 没有配置正确的话，
将会选择 VPN 创建的网络接口，而不是正确的网络接口。

所以，如果您的应用运行在 VPN 环境中，请参考上述章节，
设置正确的网络接口。

[[_when_multicast_is_not_available]]
=== 组播不可用

在某些情况下，由于特殊的运行环境，可能无法使用组播。
在这种情况下，应该配置为其他协议，例如配置 `TCPPING` 以使用 TCP 套接字，或配置 `S3_PING` 以使用亚马逊 S3。

有关其他可用的 JGroups 发现协议及其如何配置的更多信息，
请查阅 http://www.jgroups.org/manual/index.html#Discovery[JGroups文档] 。

[[_problems_with_ipv6]]
=== 使用IPv6的问题

如果在 IPv6 地址配置遇到困难，可以通过设置系统属性 `java.net.preferIPv4Stack` 请强制使用 IPv4：

----
-Djava.net.preferIPv4Stack=true
----

[[_enabling_logging]]
=== 开启日志

在排除故障时，开启 Infinispan 和 JGroups 日志很有帮助，可以观察是否组成了集群。
使用默认的 JUL 日志时，在 classpath 中添加 `vertx-default-jul-logging.properties` 文件可开启 Ignite 日志。
这是一个标准 java.util.logging（JUL） 配置文件。
具体配置如下：

----
org.infinispan.level=INFO
org.jgroups.level=INFO
----

以及

----
java.util.logging.ConsoleHandler.level=INFO
java.util.logging.FileHandler.level=INFO
----

[[_infinispan_logging]]
== Infinispan 日志配置

Infinispan 依赖与 JBoss Logging 。JBoss Logging 是一个与多种日志框架的桥接器。

请将日志框架实现的jar包放入 classpath 中，JBoss Logging 能够自动检测到并使用。

如果在 classpath 有多种日志框架，可以通过设置系统变量 `org.jboss.logging.provider` 来指定具体的实现。
例如：

----
-Dorg.jboss.logging.provider=log4j2
----

更多配置信息请参考 http://docs.jboss.org/hibernate/orm/4.3/topical/html/logging/Logging.html[JBoss日志指南] 。

[[_jgroups_logging]]
== JGroups 日志配置

JGroups 默认采用 JDK Logging 实现。同时也支持 log4j 与 log4j2 ，只要相应的 jar 包 在 classpath 中。

如果想查阅更详细的信息，或实现自己的日志后端，请参考
http://www.jgroups.org/manual/index.html#Logging[JGroups日志文档]。

== SharedData extensions

=== AsyncMap content streams

The `InfinispanAsyncMap` API allows to retrieve keys, values and entries as streams.
This can be useful if you need to go through the content of a large map for bulk processing.

[source,java]
----
InfinispanAsyncMap<K, V> infinispanAsyncMap = InfinispanAsyncMap.unwrap(asyncMap);
ReadStream<K> keyStream = infinispanAsyncMap.keyStream();
ReadStream<V> valueStream = infinispanAsyncMap.valueStream();
ReadStream<Map.Entry<K, V>> entryReadStream = infinispanAsyncMap.entryStream();
----

== Cluster administration

The Infinispan cluster manager works by turning Vert.x nodes into members of an Infinispan cluster.
As a consequence, Vert.x cluster manager administration should follow the Infinispan management guidelines.

First, let's take a step back and introduce rebalancing and split-brain syndrome.

=== Rebalancing

Each Vert.x node holds pieces of the clustering data: eventbus subscriptions, async map entries, clustered counters... etc.

When a member joins or leaves the cluster, Infinispan rebalances cache entries on the new set of members.
In other words, it moves data around to accomodate the new cluster topology.
This process may take some time, depending on the amount of clustered data and number of nodes.

=== Split-brain syndrome

In a perfect world, there would be no network equipment failures.
Reality is, though, that sooner or later your cluster will be divided into smaller groups, unable to see each others.

Infinispan is capable of merging the nodes back into a single cluster.
But just as with rebalancing, this process may take some time.
Before the cluster is fully functional again, some eventbus consumers might not be able to get messages.
Or high-availability may not be able to redeploy a failing verticle.

[NOTE]
====
It is difficult (if possible at all) to make a difference between a network partition and:

- long GC pauses (leading to missed pings),
- many nodes being killed forcefully, at-once, because you are deploying a new version of your application
====

=== Recommendations

Considering the common clustering issues discussed above, it is recommended to stick to the following good practices.

==== Graceful shutdown

Avoid stopping members forcefully (e.g, `kill -9` a node).

Of course process crashes are inevitable, but a graceful shutdown helps to get the remaining nodes in a stable state faster.

[[one-by-one]]
==== One node after the other

When rolling a new version of your app, scaling-up or down your cluster, add or remove nodes one after the other.

Stopping nodes one by one prevents the cluster from thinking a network partition occured.
Adding them one by one allows for clean, incremental rebalancing operations.

The cluster healthiness can be verified with link:../../vertx-health-check/java/[Vert.x Health Checks]:

[source,java]
----
Handler<Promise<Status>> procedure = ClusterHealthCheck.createProcedure(vertx, true);
HealthChecks checks = HealthChecks.create(vertx).register("cluster-health", procedure);
----

After creation, the health check can be exposed over HTTP with a link:../../vertx-web/java/[Vert.x Web] router handler:

[source,java]
----
Router router = Router.router(vertx);
router.get("/readiness").handler(HealthCheckHandler.createWithHealthChecks(checks));
----